{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADayInTheLife.txt            HeyJude.txt\r\n",
      "ComeTogether.txt             IWantToHoldYourHand.txt\r\n",
      "DontLetMeDown.txt            LoveMeDo.txt\r\n",
      "EleanorRigby.txt             Something.txt\r\n",
      "Help.txt                     WhileMyGuitarGentlyWeeps.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls Data/TheBeatle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Data/TheBeatle/LoveMeDo.txt',\n",
       " 'Data/TheBeatle/IWantToHoldYourHand.txt',\n",
       " 'Data/TheBeatle/WhileMyGuitarGentlyWeeps.txt',\n",
       " 'Data/TheBeatle/ComeTogether.txt',\n",
       " 'Data/TheBeatle/Something.txt',\n",
       " 'Data/TheBeatle/EleanorRigby.txt',\n",
       " 'Data/TheBeatle/ADayInTheLife.txt',\n",
       " 'Data/TheBeatle/DontLetMeDown.txt',\n",
       " 'Data/TheBeatle/Help.txt',\n",
       " 'Data/TheBeatle/HeyJude.txt',\n",
       " 'Data/TaylorSwift/BadBlood.txt',\n",
       " 'Data/TaylorSwift/YouNeedToCalmDown.txt',\n",
       " 'Data/TaylorSwift/Delicate.txt',\n",
       " 'Data/TaylorSwift/TheMan.txt',\n",
       " 'Data/TaylorSwift/YouBelongWithMe.txt',\n",
       " 'Data/TaylorSwift/SoonYouWillGetBetter.txt',\n",
       " 'Data/TaylorSwift/LookWhatYouMakeMeDo.txt',\n",
       " 'Data/TaylorSwift/Me!.txt',\n",
       " 'Data/TaylorSwift/LoveStory.txt',\n",
       " 'Data/TaylorSwift/22.txt']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load the data\n",
    "import glob\n",
    "TheBeatle=glob.glob(\"Data/TheBeatle/*.txt\")\n",
    "TaylorSwift=glob.glob(\"Data/TaylorSwift/*.txt\")\n",
    "filenames=TheBeatle+TaylorSwift\n",
    "filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Hey Jude, don't make it bad\\n\", 'Take a sad song and make it better\\n', 'Remember to let her into your heart\\n', 'Then you can start to make it better\\n']\n"
     ]
    }
   ],
   "source": [
    "#Open the test_song\n",
    "with open('Data/TheBeatle/HeyJude.txt') as f:\n",
    "    test_song = f.readlines()\n",
    "    test_song = test_song[:4]\n",
    "    print(test_song)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"You are somebody that I don't know\\n\", \"But you're takin' shots at me like it's Patr贸n\\n\", \"And I'm just like, damn, it's 7 AM\\n\", \"Say it in the street, that's a knock-out\\n\", \"But you say it in a Tweet, that's a cop-out\\n\", 'And I\\'m just like, \"Hey, are you okay?\"\\n', \"And I ain't tryna mess with your self-expression\\n\", \"But I've learned a lesson that stressin' and obsessin' 'bout somebody else is no fun\\n\", 'And snakes and stones never broke my bones\\n', 'So oh-oh, oh-oh, oh-oh, oh-oh, oh-oh\\n', \"You need to calm down, you're being too loud\\n\", \"And I'm just like oh-oh, oh-oh, oh-oh, oh-oh, oh-oh (oh)\\n\", 'You need to just stop\\n', 'Like can you just not step on my gown?\\n', 'You need to calm down\\n', \"You are somebody that we don't know\\n\", \"But you're comin' at my friends like a missile\\n\", 'Why are you mad?\\n', 'When you could be GLAAD? (You could be GLAAD)\\n', 'Sunshine on the street at the parade\\n', 'But you would rather be in the dark age\\n', \"Just makin' that sign must've taken all night\\n\", 'You just need to take several seats and then try to restore the peace\\n', 'And control your urges to scream about all the people you hate\\n', \"'Cause shade never made anybody less gay\\n\", 'So oh-oh, oh-oh, oh-oh, oh-oh, oh-oh\\n', \"You need to calm down, you're being too loud\\n\", \"And I'm just like oh-oh, oh-oh, oh-oh, oh-oh, oh-oh (oh)\\n\", 'You need to just stop\\n', 'Like can you just not step on his gown?\\n', 'You need to calm down\\n', 'And we see you over there on the internet\\n', 'Comparing all the girls who are killing it\\n', 'But we figured you out\\n', 'We all know now we all got crowns\\n', 'You need to calm down\\n', 'Oh-oh, oh-oh, oh-oh, oh-oh, oh-oh\\n', 'You need to calm down (you need to calm down)\\n', \"You're being too loud (you're being too loud)\\n\", \"And I'm just like oh-oh, oh-oh, oh-oh, oh-oh, oh-oh (oh)\\n\", 'You need to just stop (can you stop?)\\n', 'Like can you just not step on our gowns?\\n', 'You need to calm down']\n"
     ]
    }
   ],
   "source": [
    "with open('Data/TaylorSwift/YouNeedToCalmDown.txt') as f:\n",
    "    test_song = f.readlines()\n",
    "    test_song = test_song\n",
    "    print(test_song)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Filtering and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"you are somebody that i don't know\",\n",
       " \"but you're takin' shots at me like it's patr贸n\",\n",
       " \"and i'm just like damn it's 7 am\",\n",
       " \"say it in the street that's a knockout\"]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "punctuations = list(string.punctuation)\n",
    "punctuations.remove(\"'\") #Not include the apostrophe\n",
    "punctuations+=\"\\n\"\n",
    "def clean_song(song):\n",
    "    cleaned = []\n",
    "    for line in song:\n",
    "        for symbol in punctuations:\n",
    "            line = line.replace(symbol, '').lower()\n",
    "        cleaned.append(line)\n",
    "    return cleaned\n",
    "\n",
    "clean_song(test_song)[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', \"'m\", 'dangerous', '.', 'Do', \"n't\", 'underestimate', '!', '!', '!']\n",
      "[\"I'm\", 'dangerous.', \"Don't\", 'underestimate!!!']\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "sentence= \"I'm dangerous. Don't underestimate!!!\"\n",
    "print(word_tokenize(sentence))\n",
    "print(sentence.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['You',\n",
       " 'are',\n",
       " 'somebody',\n",
       " 'that',\n",
       " 'I',\n",
       " 'do',\n",
       " \"n't\",\n",
       " 'know',\n",
       " 'But',\n",
       " 'you',\n",
       " \"'re\",\n",
       " 'takin',\n",
       " \"'\",\n",
       " 'shots',\n",
       " 'at',\n",
       " 'me',\n",
       " 'like',\n",
       " 'it',\n",
       " \"'s\",\n",
       " 'Patr贸n',\n",
       " 'And',\n",
       " 'I',\n",
       " \"'m\",\n",
       " 'just',\n",
       " 'like',\n",
       " ',',\n",
       " 'damn',\n",
       " ',',\n",
       " 'it',\n",
       " \"'s\",\n",
       " '7',\n",
       " 'AM',\n",
       " 'Say',\n",
       " 'it',\n",
       " 'in',\n",
       " 'the',\n",
       " 'street',\n",
       " ',',\n",
       " 'that',\n",
       " \"'s\",\n",
       " 'a',\n",
       " 'knock-out',\n",
       " 'But',\n",
       " 'you',\n",
       " 'say',\n",
       " 'it',\n",
       " 'in',\n",
       " 'a',\n",
       " 'Tweet',\n",
       " ',',\n",
       " 'that',\n",
       " \"'s\",\n",
       " 'a',\n",
       " 'cop-out',\n",
       " 'And',\n",
       " 'I',\n",
       " \"'m\",\n",
       " 'just',\n",
       " 'like',\n",
       " ',',\n",
       " '``',\n",
       " 'Hey',\n",
       " ',',\n",
       " 'are',\n",
       " 'you',\n",
       " 'okay',\n",
       " '?',\n",
       " \"''\",\n",
       " 'And',\n",
       " 'I',\n",
       " 'ai',\n",
       " \"n't\",\n",
       " 'tryna',\n",
       " 'mess',\n",
       " 'with',\n",
       " 'your',\n",
       " 'self-expression',\n",
       " 'But',\n",
       " 'I',\n",
       " \"'ve\",\n",
       " 'learned',\n",
       " 'a',\n",
       " 'lesson',\n",
       " 'that',\n",
       " 'stressin',\n",
       " \"'\",\n",
       " 'and',\n",
       " 'obsessin',\n",
       " \"'\",\n",
       " \"'bout\",\n",
       " 'somebody',\n",
       " 'else',\n",
       " 'is',\n",
       " 'no',\n",
       " 'fun',\n",
       " 'And',\n",
       " 'snakes',\n",
       " 'and',\n",
       " 'stones',\n",
       " 'never',\n",
       " 'broke',\n",
       " 'my',\n",
       " 'bones',\n",
       " 'So',\n",
       " 'oh-oh',\n",
       " ',',\n",
       " 'oh-oh',\n",
       " ',',\n",
       " 'oh-oh',\n",
       " ',',\n",
       " 'oh-oh',\n",
       " ',',\n",
       " 'oh-oh',\n",
       " 'You',\n",
       " 'need',\n",
       " 'to',\n",
       " 'calm',\n",
       " 'down',\n",
       " ',',\n",
       " 'you',\n",
       " \"'re\",\n",
       " 'being',\n",
       " 'too',\n",
       " 'loud',\n",
       " 'And',\n",
       " 'I',\n",
       " \"'m\",\n",
       " 'just',\n",
       " 'like',\n",
       " 'oh-oh',\n",
       " ',',\n",
       " 'oh-oh',\n",
       " ',',\n",
       " 'oh-oh',\n",
       " ',',\n",
       " 'oh-oh',\n",
       " ',',\n",
       " 'oh-oh',\n",
       " '(',\n",
       " 'oh',\n",
       " ')',\n",
       " 'You',\n",
       " 'need',\n",
       " 'to',\n",
       " 'just',\n",
       " 'stop',\n",
       " 'Like',\n",
       " 'can',\n",
       " 'you',\n",
       " 'just',\n",
       " 'not',\n",
       " 'step',\n",
       " 'on',\n",
       " 'my',\n",
       " 'gown',\n",
       " '?',\n",
       " 'You',\n",
       " 'need',\n",
       " 'to',\n",
       " 'calm',\n",
       " 'down',\n",
       " 'You',\n",
       " 'are',\n",
       " 'somebody',\n",
       " 'that',\n",
       " 'we',\n",
       " 'do',\n",
       " \"n't\",\n",
       " 'know',\n",
       " 'But',\n",
       " 'you',\n",
       " \"'re\",\n",
       " 'comin',\n",
       " \"'\",\n",
       " 'at',\n",
       " 'my',\n",
       " 'friends',\n",
       " 'like',\n",
       " 'a',\n",
       " 'missile',\n",
       " 'Why',\n",
       " 'are',\n",
       " 'you',\n",
       " 'mad',\n",
       " '?',\n",
       " 'When',\n",
       " 'you',\n",
       " 'could',\n",
       " 'be',\n",
       " 'GLAAD',\n",
       " '?',\n",
       " '(',\n",
       " 'You',\n",
       " 'could',\n",
       " 'be',\n",
       " 'GLAAD',\n",
       " ')',\n",
       " 'Sunshine',\n",
       " 'on',\n",
       " 'the',\n",
       " 'street',\n",
       " 'at',\n",
       " 'the',\n",
       " 'parade',\n",
       " 'But',\n",
       " 'you',\n",
       " 'would',\n",
       " 'rather',\n",
       " 'be',\n",
       " 'in',\n",
       " 'the',\n",
       " 'dark',\n",
       " 'age',\n",
       " 'Just',\n",
       " 'makin',\n",
       " \"'\",\n",
       " 'that',\n",
       " 'sign',\n",
       " 'must',\n",
       " \"'ve\",\n",
       " 'taken',\n",
       " 'all',\n",
       " 'night',\n",
       " 'You',\n",
       " 'just',\n",
       " 'need',\n",
       " 'to',\n",
       " 'take',\n",
       " 'several',\n",
       " 'seats',\n",
       " 'and',\n",
       " 'then',\n",
       " 'try',\n",
       " 'to',\n",
       " 'restore',\n",
       " 'the',\n",
       " 'peace',\n",
       " 'And',\n",
       " 'control',\n",
       " 'your',\n",
       " 'urges',\n",
       " 'to',\n",
       " 'scream',\n",
       " 'about',\n",
       " 'all',\n",
       " 'the',\n",
       " 'people',\n",
       " 'you',\n",
       " 'hate',\n",
       " \"'Cause\",\n",
       " 'shade',\n",
       " 'never',\n",
       " 'made',\n",
       " 'anybody',\n",
       " 'less',\n",
       " 'gay',\n",
       " 'So',\n",
       " 'oh-oh',\n",
       " ',',\n",
       " 'oh-oh',\n",
       " ',',\n",
       " 'oh-oh',\n",
       " ',',\n",
       " 'oh-oh',\n",
       " ',',\n",
       " 'oh-oh',\n",
       " 'You',\n",
       " 'need',\n",
       " 'to',\n",
       " 'calm',\n",
       " 'down',\n",
       " ',',\n",
       " 'you',\n",
       " \"'re\",\n",
       " 'being',\n",
       " 'too',\n",
       " 'loud',\n",
       " 'And',\n",
       " 'I',\n",
       " \"'m\",\n",
       " 'just',\n",
       " 'like',\n",
       " 'oh-oh',\n",
       " ',',\n",
       " 'oh-oh',\n",
       " ',',\n",
       " 'oh-oh',\n",
       " ',',\n",
       " 'oh-oh',\n",
       " ',',\n",
       " 'oh-oh',\n",
       " '(',\n",
       " 'oh',\n",
       " ')',\n",
       " 'You',\n",
       " 'need',\n",
       " 'to',\n",
       " 'just',\n",
       " 'stop',\n",
       " 'Like',\n",
       " 'can',\n",
       " 'you',\n",
       " 'just',\n",
       " 'not',\n",
       " 'step',\n",
       " 'on',\n",
       " 'his',\n",
       " 'gown',\n",
       " '?',\n",
       " 'You',\n",
       " 'need',\n",
       " 'to',\n",
       " 'calm',\n",
       " 'down',\n",
       " 'And',\n",
       " 'we',\n",
       " 'see',\n",
       " 'you',\n",
       " 'over',\n",
       " 'there',\n",
       " 'on',\n",
       " 'the',\n",
       " 'internet',\n",
       " 'Comparing',\n",
       " 'all',\n",
       " 'the',\n",
       " 'girls',\n",
       " 'who',\n",
       " 'are',\n",
       " 'killing',\n",
       " 'it',\n",
       " 'But',\n",
       " 'we',\n",
       " 'figured',\n",
       " 'you',\n",
       " 'out',\n",
       " 'We',\n",
       " 'all',\n",
       " 'know',\n",
       " 'now',\n",
       " 'we',\n",
       " 'all',\n",
       " 'got',\n",
       " 'crowns',\n",
       " 'You',\n",
       " 'need',\n",
       " 'to',\n",
       " 'calm',\n",
       " 'down',\n",
       " 'Oh-oh',\n",
       " ',',\n",
       " 'oh-oh',\n",
       " ',',\n",
       " 'oh-oh',\n",
       " ',',\n",
       " 'oh-oh',\n",
       " ',',\n",
       " 'oh-oh',\n",
       " 'You',\n",
       " 'need',\n",
       " 'to',\n",
       " 'calm',\n",
       " 'down',\n",
       " '(',\n",
       " 'you',\n",
       " 'need',\n",
       " 'to',\n",
       " 'calm',\n",
       " 'down',\n",
       " ')',\n",
       " 'You',\n",
       " \"'re\",\n",
       " 'being',\n",
       " 'too',\n",
       " 'loud',\n",
       " '(',\n",
       " 'you',\n",
       " \"'re\",\n",
       " 'being',\n",
       " 'too',\n",
       " 'loud',\n",
       " ')',\n",
       " 'And',\n",
       " 'I',\n",
       " \"'m\",\n",
       " 'just',\n",
       " 'like',\n",
       " 'oh-oh',\n",
       " ',',\n",
       " 'oh-oh',\n",
       " ',',\n",
       " 'oh-oh',\n",
       " ',',\n",
       " 'oh-oh',\n",
       " ',',\n",
       " 'oh-oh',\n",
       " '(',\n",
       " 'oh',\n",
       " ')',\n",
       " 'You',\n",
       " 'need',\n",
       " 'to',\n",
       " 'just',\n",
       " 'stop',\n",
       " '(',\n",
       " 'can',\n",
       " 'you',\n",
       " 'stop',\n",
       " '?',\n",
       " ')',\n",
       " 'Like',\n",
       " 'can',\n",
       " 'you',\n",
       " 'just',\n",
       " 'not',\n",
       " 'step',\n",
       " 'on',\n",
       " 'our',\n",
       " 'gowns',\n",
       " '?',\n",
       " 'You',\n",
       " 'need',\n",
       " 'to',\n",
       " 'calm',\n",
       " 'down']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#https://stackoverflow.com/questions/35345761/python-re-split-vs-nltk-word-tokenize-and-sent-tokenize\n",
    "def tokenize(song):\n",
    "    joined_song = ' '.join(song) #Join sentences together\n",
    "    tokenized_song = word_tokenize(joined_song)\n",
    "    return tokenized_song\n",
    "\n",
    "tokenize(test_song)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming, Lemmatization, and Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "takin' : takin' takin'\n",
      "knock-out : knock-out knock-out\n",
      "don't : don't don't\n",
      "dangerous : danger dangerous\n",
      "working : work work\n",
      "apples : appl apples\n",
      "better : better better\n",
      "died : die die\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer  \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "   \n",
    "ps = PorterStemmer() \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "# choose some words to be stemmed \n",
    "words = [\"takin'\", \"knock-out\", \"don't\", \"dangerous\",\"working\", \"apples\", 'better', 'died' ] \n",
    "  \n",
    "for w in words: \n",
    "    print(w,\":\", ps.stem(w),lemmatizer.lemmatize(w,pos =\"v\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]\n",
      "['what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords_list = stopwords.words('english')\n",
    "print(len(stopwords_list))\n",
    "print(stopwords_list[:10])\n",
    "print(stopwords_list[35:45])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['You', 'somebody', 'I', 'know', 'But', 'you', 'takin', 'shoot', 'me', 'like', 'Patr贸n', 'And', 'I', 'like', ',', 'damn', ',', '7', 'AM', 'Say', 'street', ',', 'knock-out']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "stopwords_list = stopwords.words('english')\n",
    "stopwords_list=stopwords_list[35:]\n",
    "stopwords_list+=[\"n't\", \"it\", \"'s\",\"'m\", \"'re\",\"'\"]\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize_remove_stopwords(tokenize_list):\n",
    "    lemmatized_words=[]\n",
    "    for w in tokenize_list:\n",
    "        w=lemmatizer.lemmatize(w,pos =\"a\")\n",
    "        w=lemmatizer.lemmatize(w,pos =\"n\")\n",
    "        w=lemmatizer.lemmatize(w,pos =\"v\")\n",
    "        lemmatized_words.append(w)\n",
    "    song_noStopword = [i for i in lemmatized_words if i not in stopwords_list]\n",
    "    return song_noStopword\n",
    "\n",
    "print(lemmatize_remove_stopwords(tokenize(test_song))[:23])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize and frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_and_freq(song, vocab=None):\n",
    "    \n",
    "    if vocab:\n",
    "        unique_words = vocab\n",
    "    else:\n",
    "        unique_words = list(set(song))\n",
    "    #initial bag of word dictionary\n",
    "    song_dict = {i:0 for i in unique_words} \n",
    "    \n",
    "    for word in song:\n",
    "        song_dict[word] += 1\n",
    "    \n",
    "    return song_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('le', 1), ('But', 6), ('okay', 1), ('bone', 1), ('calm', 8), ('When', 1), ('must', 1), ('your', 2), ('see', 1), ('like', 7)]\n"
     ]
    }
   ],
   "source": [
    "output_=lemmatize_remove_stopwords(tokenize(test_song))\n",
    "output_=vectorize_and_freq(output_)\n",
    "print(list(output_.items())[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Term frequency - Inverse Document Frequency (TF-IDF)\n",
    "\n",
    "$$\\text{TF-IDF}=\\text{TF}\\times \\text{IDF}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{TF }(t) = \\frac{\\text{number of times t appears in a document}} {\\text{total number of terms in the document}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def term_frequency(BoW_dict):\n",
    "    total_word_count = sum(BoW_dict.values())\n",
    "    \n",
    "    for ind, val in BoW_dict.items():\n",
    "        BoW_dict[ind] = val/ total_word_count\n",
    "    \n",
    "    return BoW_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('you', 0.1542056074766355)\n",
      "('ohoh', 0.14018691588785046)\n",
      "('need', 0.056074766355140186)\n",
      "('like', 0.04672897196261682)\n",
      "('i', 0.037383177570093455)\n",
      "('calm', 0.037383177570093455)\n",
      "('we', 0.02336448598130841)\n",
      "('stop', 0.018691588785046728)\n",
      "('loud', 0.018691588785046728)\n",
      "('step', 0.014018691588785047)\n"
     ]
    }
   ],
   "source": [
    "output_=clean_song(test_song)\n",
    "output_=tokenize(output_)\n",
    "output_=lemmatize_remove_stopwords(output_)\n",
    "output_=vectorize_and_freq(output_)\n",
    "ouput_=term_frequency(output_)\n",
    "\n",
    "ouput_=sorted(ouput_.items(), key = lambda kv:(kv[1], kv[0]),reverse=True)\n",
    "\n",
    "for i in ouput_[:10]:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inverse Document Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{IDF}(t) =  log_e \\left ( \\frac{\\text{Total Number of Documents}}{\\text{Number of Documents with t in it}} \\right ) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_document_frequency(dicts_list):\n",
    "    #0 : Totoal Number of Document\n",
    "    num_doc=len(dicts_list)\n",
    "    \n",
    "    #1: Find all the unique words for all the document\n",
    "    vocab_set=[]\n",
    "    for d in dicts_list:\n",
    "        for word in d.keys():\n",
    "            vocab_set.append(word)\n",
    "    vocab_set=set(vocab_set)\n",
    "    \n",
    "    #2: Number of document with t in it\n",
    "    idf_t = {i:0 for i in vocab_set} #\n",
    "    \n",
    "    for word in idf_t.keys():\n",
    "        docs = 0\n",
    "        # Find number of doc for each word\n",
    "        for d in dicts_list:\n",
    "            if word in d:\n",
    "                docs += 1\n",
    "        \n",
    "        # Compute idf for each t\n",
    "        idf_t[word] = np.log((num_doc/ float(docs)))\n",
    "    \n",
    "    return idf_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf(dicts_list):\n",
    "    # Vocab for corpus\n",
    "    doc_tf_idf = {}\n",
    "    idf = inverse_document_frequency(dicts_list)\n",
    "    full_vocab_list = {i:0 for i in list(idf.keys())}\n",
    "    \n",
    "    # Create tf-idf list of dictionaries, containing a dictionary that will be updated for each document\n",
    "    tf_idf_list_of_dicts = []\n",
    "    \n",
    "    # Now, compute tf and then use this to compute and set tf-idf values for each document\n",
    "    for doc in dicts_list:\n",
    "        doc_tf = term_frequency(doc)\n",
    "        for word in doc_tf:\n",
    "            doc_tf_idf[word] = doc_tf[word] * idf[word]\n",
    "        tf_idf_list_of_dicts.append(doc_tf_idf)\n",
    "    return tf_idf_list_of_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Dimensions: 714\n",
      "want 0.015178405413721431\n",
      "ca 0.007717774386704719\n",
      "something 0.006311521225231766\n",
      "happy 0.016807190459810554\n",
      "touch 0.008591735421619574\n"
     ]
    }
   ],
   "source": [
    "def main(filenames):\n",
    "    # Iterate through list of filenames and read each in\n",
    "    cvad = [] #count vectorized all documents\n",
    "    for file in filenames:\n",
    "        with open(file) as f:\n",
    "            raw_data = f.readlines()\n",
    "        # Clean and tokenize raw text\n",
    "        ouput_ = clean_song(raw_data)\n",
    "        ouput_ = tokenize(ouput_)\n",
    "        ouput_=lemmatize_remove_stopwords(ouput_)  \n",
    "        ouput_ = vectorize_and_freq(ouput_)\n",
    "        cvad.append(ouput_)\n",
    "    # tf-idf representation of everything\n",
    "    tf_idf_all_docs = tf_idf(cvad)\n",
    "    \n",
    "    return tf_idf_all_docs\n",
    "tf_idf_all_docs = main(filenames)\n",
    "print(\"Number of Dimensions: {}\".format(len(tf_idf_all_docs[12])))\n",
    "for key, value in list(tf_idf_all_docs[15].items())[20:25]:\n",
    "    print(key,value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Reduce dimentionality\n",
    "tf_idf_vals_list = []\n",
    "\n",
    "tf_idf_vals_list= [list(i.values()) for i in tf_idf_all_docs]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "np.random.seed(4)\n",
    "tf_idf_all_docs = main(filenames)\n",
    "tf_idf_vals_list= [list(i.values()) for i in tf_idf_all_docs] \n",
    "TSNE_2d = TSNE(n_components=2)\n",
    "data_2d = TSNE_2d.fit_transform(tf_idf_vals_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "np.random.seed(10)\n",
    "\n",
    "tf_idf_all_docs = main(filenames) \n",
    "TSNE_2d = TSNE(n_components=2)\n",
    "data_2d = TSNE_2d.fit_transform(tf_idf_vals_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "TheBeatle = data_2d[:10]\n",
    "beatle_x = [i[0] for i in TheBeatle]\n",
    "beatle_y = [i[1] for i in TheBeatle]\n",
    "\n",
    "TaylorSwift = data_2d[10:]\n",
    "ts_x = [i[0] for i in TaylorSwift]\n",
    "ts_y = [i[1] for i in TaylorSwift]\n",
    "\n",
    "fig = plt.figure(figsize=(20,10))\n",
    "ax = fig.add_subplot(222)\n",
    "ax.scatter(beatle_x, beatle_y, color='g', label='The Beatles')\n",
    "ax.scatter(ts_x, ts_y, color='r', label='Taylor Swift')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
