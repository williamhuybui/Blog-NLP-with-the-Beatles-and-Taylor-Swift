{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADayInTheLife.txt            HeyJude.txt\r\n",
      "ComeTogether.txt             IWantToHoldYourHand.txt\r\n",
      "DontLetMeDown.txt            LoveMeDo.txt\r\n",
      "EleanorRigby.txt             Something.txt\r\n",
      "Help.txt                     WhileMyGuitarGentlyWeeps.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls Data/TheBeatle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Data/TheBeatle/LoveMeDo.txt',\n",
       " 'Data/TheBeatle/IWantToHoldYourHand.txt',\n",
       " 'Data/TheBeatle/WhileMyGuitarGentlyWeeps.txt',\n",
       " 'Data/TheBeatle/ComeTogether.txt',\n",
       " 'Data/TheBeatle/Something.txt',\n",
       " 'Data/TheBeatle/EleanorRigby.txt',\n",
       " 'Data/TheBeatle/ADayInTheLife.txt',\n",
       " 'Data/TheBeatle/DontLetMeDown.txt',\n",
       " 'Data/TheBeatle/Help.txt',\n",
       " 'Data/TheBeatle/HeyJude.txt',\n",
       " 'Data/TaylorSwift/BadBlood.txt',\n",
       " 'Data/TaylorSwift/YouNeedToCalmDown.txt',\n",
       " 'Data/TaylorSwift/Delicate.txt',\n",
       " 'Data/TaylorSwift/TheMan.txt',\n",
       " 'Data/TaylorSwift/YouBelongWithMe.txt',\n",
       " 'Data/TaylorSwift/SoonYouWillGetBetter.txt',\n",
       " 'Data/TaylorSwift/LookWhatYouMakeMeDo.txt',\n",
       " 'Data/TaylorSwift/Me!.txt',\n",
       " 'Data/TaylorSwift/LoveStory.txt',\n",
       " 'Data/TaylorSwift/22.txt']"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load the data\n",
    "import glob\n",
    "TheBeatle=glob.glob(\"Data/TheBeatle/*.txt\")\n",
    "TaylorSwift=glob.glob(\"Data/TaylorSwift/*.txt\")\n",
    "filenames=TheBeatle+TaylorSwift\n",
    "filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Hey Jude, don't make it bad\\n\", 'Take a sad song and make it better\\n', 'Remember to let her into your heart\\n', 'Then you can start to make it better\\n']\n"
     ]
    }
   ],
   "source": [
    "#Open the test_song\n",
    "with open('Data/TheBeatle/HeyJude.txt') as f:\n",
    "    test_song = f.readlines()\n",
    "    test_song = test_song[:4]\n",
    "    print(test_song)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"You are somebody that I don't know\\n\", \"But you're takin' shots at me like it's Patrón\\n\", \"And I'm just like, damn, it's 7 AM\\n\", \"Say it in the street, that's a knock-out\\n\", \"But you say it in a Tweet, that's a cop-out\\n\", 'And I\\'m just like, \"Hey, are you okay?\"\\n', \"And I ain't tryna mess with your self-expression\\n\", \"But I've learned a lesson that stressin' and obsessin' 'bout somebody else is no fun\\n\", 'And snakes and stones never broke my bones\\n', 'So oh-oh, oh-oh, oh-oh, oh-oh, oh-oh\\n', \"You need to calm down, you're being too loud\\n\", \"And I'm just like oh-oh, oh-oh, oh-oh, oh-oh, oh-oh (oh)\\n\", 'You need to just stop\\n', 'Like can you just not step on my gown?\\n', 'You need to calm down\\n', \"You are somebody that we don't know\\n\", \"But you're comin' at my friends like a missile\\n\", 'Why are you mad?\\n', 'When you could be GLAAD? (You could be GLAAD)\\n', 'Sunshine on the street at the parade\\n', 'But you would rather be in the dark age\\n', \"Just makin' that sign must've taken all night\\n\", 'You just need to take several seats and then try to restore the peace\\n', 'And control your urges to scream about all the people you hate\\n', \"'Cause shade never made anybody less gay\\n\", 'So oh-oh, oh-oh, oh-oh, oh-oh, oh-oh\\n', \"You need to calm down, you're being too loud\\n\", \"And I'm just like oh-oh, oh-oh, oh-oh, oh-oh, oh-oh (oh)\\n\", 'You need to just stop\\n', 'Like can you just not step on his gown?\\n', 'You need to calm down\\n', 'And we see you over there on the internet\\n', 'Comparing all the girls who are killing it\\n', 'But we figured you out\\n', 'We all know now we all got crowns\\n', 'You need to calm down\\n', 'Oh-oh, oh-oh, oh-oh, oh-oh, oh-oh\\n', 'You need to calm down (you need to calm down)\\n', \"You're being too loud (you're being too loud)\\n\", \"And I'm just like oh-oh, oh-oh, oh-oh, oh-oh, oh-oh (oh)\\n\", 'You need to just stop (can you stop?)\\n', 'Like can you just not step on our gowns?\\n', 'You need to calm down']\n"
     ]
    }
   ],
   "source": [
    "with open('Data/TaylorSwift/YouNeedToCalmDown.txt') as f:\n",
    "    test_song = f.readlines()\n",
    "    test_song = test_song\n",
    "    print(test_song)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['!',\n",
       " '\"',\n",
       " '#',\n",
       " '$',\n",
       " '%',\n",
       " '&',\n",
       " '(',\n",
       " ')',\n",
       " '*',\n",
       " '+',\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '/',\n",
       " ':',\n",
       " ';',\n",
       " '<',\n",
       " '=',\n",
       " '>',\n",
       " '?',\n",
       " '@',\n",
       " '[',\n",
       " '\\\\',\n",
       " ']',\n",
       " '^',\n",
       " '_',\n",
       " '`',\n",
       " '{',\n",
       " '|',\n",
       " '}',\n",
       " '~']"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "punctuations = list(string.punctuation)\n",
    "punctuations.remove(\"'\")\n",
    "punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean\n",
    "import string\n",
    "\n",
    "punctuations = list(string.punctuation)\n",
    "punctuations.remove(\"'\")\n",
    "punctuations+=\"\\n\"\n",
    "\n",
    "def clean_song(song):\n",
    "    cleaned = []\n",
    "    for line in song:\n",
    "        for symbol in punctuations:\n",
    "            line = line.replace(symbol, '').lower()\n",
    "        cleaned.append(line)\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"you are somebody that i don't know\",\n",
       " \"but you're takin' shots at me like it's patrón\",\n",
       " \"and i'm just like damn it's 7 am\",\n",
       " \"say it in the street that's a knockout\"]"
      ]
     },
     "execution_count": 372,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_song=clean_song(test_song)\n",
    "test_song"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', \"'m\", 'dangerous', '.', 'Do', \"n't\", 'underestimate', '!', '!', '!']\n",
      "[\"I'm\", 'dangerous.', \"Don't\", 'underestimate!!!']\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "sentence= \"I'm dangerous. Don't underestimate!!!\"\n",
    "print(word_tokenize(sentence))\n",
    "print(sentence.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/35345761/python-re-split-vs-nltk-word-tokenize-and-sent-tokenize\n",
    "def tokenize(song):\n",
    "    joined_song = ' '.join(song)\n",
    "    tokenized_song = word_tokenize(joined_song)\n",
    "    return tokenized_song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['you',\n",
       " 'are',\n",
       " 'somebody',\n",
       " 'that',\n",
       " 'i',\n",
       " 'do',\n",
       " \"n't\",\n",
       " 'know',\n",
       " 'but',\n",
       " 'you',\n",
       " \"'re\",\n",
       " 'takin',\n",
       " \"'\",\n",
       " 'shots',\n",
       " 'at',\n",
       " 'me',\n",
       " 'like',\n",
       " 'it',\n",
       " \"'s\",\n",
       " 'patrón',\n",
       " 'and',\n",
       " 'i',\n",
       " \"'m\",\n",
       " 'just',\n",
       " 'like',\n",
       " 'damn',\n",
       " 'it',\n",
       " \"'s\",\n",
       " '7',\n",
       " 'am',\n",
       " 'say',\n",
       " 'it',\n",
       " 'in',\n",
       " 'the',\n",
       " 'street',\n",
       " 'that',\n",
       " \"'s\",\n",
       " 'a',\n",
       " 'knockout']"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize(test_song)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming, Lemmatization, and Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "takin' : takin' takin'\n",
      "knock-out : knock-out knock-out\n",
      "don't : don't don't\n",
      "dangerous : danger dangerous\n",
      "working : work working\n",
      "apples : appl apples\n",
      "better : better good\n",
      "died : die died\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "   \n",
    "ps = PorterStemmer() \n",
    "lemmatizer = WordNetLemmatizer() \n",
    "\n",
    "# choose some words to be stemmed \n",
    "words = [\"takin'\", \"knock-out\", \"don't\", \"dangerous\", \"working\", \"apples\", 'better', 'died' ] \n",
    "  \n",
    "for w in words: \n",
    "    print(w,\":\", ps.stem(w),lemmatizer.lemmatize(w,pos =\"a\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]\n",
      "['what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords_list = stopwords.words('english')\n",
    "print(len(stopwords_list))\n",
    "print(stopwords_list[:10])\n",
    "print(stopwords_list[35:45])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "stopwords_list = stopwords.words('english')\n",
    "stopwords_list=stopwords_list[35:]\n",
    "stopwords_list+=[\"n't\", \"it\", \"'s\",\"'m\", \"'re\",\"'\",\"'ll\",\"'d\"]\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "\n",
    "def lemmatize_remove_stopwords(tokenize_list):\n",
    "    lemmatized_words=[]\n",
    "    for w in tokenize_list:\n",
    "        w=lemmatizer.lemmatize(w,pos =\"a\")\n",
    "        w=lemmatizer.lemmatize(w,pos =\"n\")\n",
    "        w=lemmatizer.lemmatize(w,pos =\"v\")\n",
    "        lemmatized_words.append(w)\n",
    "    song_noStopword = [i for i in lemmatized_words if i not in stopwords_list]\n",
    "    return song_noStopword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"you are somebody that i don't know\",\n",
       " \"but you're takin' shots at me like it's patrón\",\n",
       " \"and i'm just like damn it's 7 am\",\n",
       " \"say it in the street that's a knockout\"]"
      ]
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['you', 'somebody', 'i', 'know', 'you', 'takin', 'shoot', 'me', 'like', 'patrón', 'i', 'like', 'damn', '7', 'say', 'street', 'knockout']\n"
     ]
    }
   ],
   "source": [
    "song_noStopword=lemmatize_remove_stopwords(tokenize(test_song))\n",
    "print(song_noStopword)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize and frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "song=song_noStopword\n",
    "vocab=None\n",
    "if vocab:\n",
    "    unique_words = vocab\n",
    "else:\n",
    "    unique_words = list(set(song))\n",
    "\n",
    "#Initialize the dictionary\n",
    "song_dict = {i:0 for i in unique_words}\n",
    "\n",
    "for word in song:\n",
    "    song_dict[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'you': 2,\n",
       " '7': 1,\n",
       " 'somebody': 1,\n",
       " 'say': 1,\n",
       " 'street': 1,\n",
       " 'like': 2,\n",
       " 'knockout': 1,\n",
       " 'know': 1,\n",
       " 'shoot': 1,\n",
       " 'patrón': 1,\n",
       " 'i': 2,\n",
       " 'damn': 1,\n",
       " 'me': 1,\n",
       " 'takin': 1}"
      ]
     },
     "execution_count": 378,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "song_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'you': 2, '7': 1, 'somebody': 1, 'say': 1, 'street': 1, 'like': 2, 'knockout': 1, 'know': 1, 'shoot': 1, 'patrón': 1, 'i': 2, 'damn': 1, 'me': 1, 'takin': 1}\n"
     ]
    }
   ],
   "source": [
    "def vectorize_and_freq(song, vocab=None):\n",
    "    \n",
    "    if vocab:\n",
    "        unique_words = vocab\n",
    "    else:\n",
    "        unique_words = list(set(song))\n",
    "    \n",
    "    song_dict = {i:0 for i in unique_words}\n",
    "    \n",
    "    for word in song:\n",
    "        song_dict[word] += 1\n",
    "    \n",
    "    return song_dict\n",
    "\n",
    "test_vectorized = vectorize_and_freq(song_noStopword)\n",
    "print(test_vectorized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Term frequency - Inverse Document Frequency (TF-IDF)\n",
    "\n",
    "$$\\text{TF-IDF}=\\text{TF}\\times \\text{IDF}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{TF }(t) = \\frac{\\text{number of times t appears in a document}} {\\text{total number of terms in the document}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF\n",
    "def term_frequency(BoW_dict):\n",
    "    total_word_count = sum(BoW_dict.values())\n",
    "    \n",
    "    for ind, val in BoW_dict.items():\n",
    "        BoW_dict[ind] = val/ total_word_count\n",
    "    \n",
    "    return BoW_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('you', 0.1542056074766355)\n",
      "('ohoh', 0.14018691588785046)\n",
      "('need', 0.056074766355140186)\n",
      "('like', 0.04672897196261682)\n",
      "('i', 0.037383177570093455)\n",
      "('calm', 0.037383177570093455)\n",
      "('we', 0.02336448598130841)\n",
      "('stop', 0.018691588785046728)\n",
      "('loud', 0.018691588785046728)\n",
      "('step', 0.014018691588785047)\n"
     ]
    }
   ],
   "source": [
    "output_=clean_song(test_song)\n",
    "output_=tokenize(output_)\n",
    "output_=lemmatize_remove_stopwords(output_)\n",
    "output_=vectorize_and_freq(output_)\n",
    "ouput_=term_frequency(output_)\n",
    "\n",
    "ouput_=sorted(ouput_.items(), key = lambda kv:(kv[1], kv[0]),reverse=True)\n",
    "\n",
    "for i in ouput_[:10]:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inverse Document Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{IDF}(t) =  log_e \\left ( \\frac{\\text{Total Number of Documents}}{\\text{Number of Documents with t in it}} \\right ) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_document_frequency(dicts_list):\n",
    "    #0 : Totoal Number of Document\n",
    "    num_doc=len(dicts_list)\n",
    "    \n",
    "    #1: Find all the unique words for all the document\n",
    "    vocab_set=[]\n",
    "    for d in dicts_list:\n",
    "        for word in d.keys():\n",
    "            vocab_set.append(word)\n",
    "    vocab_set=set(vocab_set)\n",
    "    \n",
    "    #2: Number of document with t in it\n",
    "    idf_t = {i:0 for i in vocab_set} #\n",
    "    \n",
    "    for word in idf_t.keys():\n",
    "        docs = 0\n",
    "        # Find number of doc for each word\n",
    "        for d in dicts_list:\n",
    "            if word in d:\n",
    "                docs += 1\n",
    "        \n",
    "        # Compute idf for each t\n",
    "        idf_t[word] = np.log((num_doc/ float(docs)))\n",
    "    \n",
    "    return idf_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf(dicts_list):\n",
    "    # Vocab for corpus\n",
    "    doc_tf_idf = {}\n",
    "    idf = inverse_document_frequency(dicts_list)\n",
    "    full_vocab_list = {i:0 for i in list(idf.keys())}\n",
    "    \n",
    "    # Create tf-idf list of dictionaries, containing a dictionary that will be updated for each document\n",
    "    tf_idf_list_of_dicts = []\n",
    "    \n",
    "    # Now, compute tf and then use this to compute and set tf-idf values for each document\n",
    "    for doc in dicts_list:\n",
    "        doc_tf = term_frequency(doc)\n",
    "        for word in doc_tf:\n",
    "            doc_tf_idf[word] = doc_tf[word] * idf[word]\n",
    "        tf_idf_list_of_dicts.append(doc_tf_idf)\n",
    "    return tf_idf_list_of_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Dimensions: 712\n",
      "let 0.003921735518976991\n",
      "happy 0.016807190459810554\n",
      "get 0.006288801694780355\n",
      "your 0.01002310915754525\n",
      "think 0.0033410363858484165\n"
     ]
    }
   ],
   "source": [
    "def main(filenames):\n",
    "    # Iterate through list of filenames and read each in\n",
    "    cvad = [] #count vectorized all documents\n",
    "    for file in filenames:\n",
    "        with open(file) as f:\n",
    "            raw_data = f.readlines()\n",
    "        # Clean and tokenize raw text\n",
    "        ouput_ = clean_song(raw_data)\n",
    "        ouput_ = tokenize(ouput_)\n",
    "        ouput_=lemmatize_remove_stopwords(ouput_)  \n",
    "        ouput_ = vectorize_and_freq(ouput_)\n",
    "        cvad.append(ouput_)\n",
    "    # tf-idf representation of everything\n",
    "    tf_idf_all_docs = tf_idf(cvad)\n",
    "    \n",
    "    return tf_idf_all_docs\n",
    "\n",
    "tf_idf_all_docs = main(filenames)\n",
    "print(\"Number of Dimensions: {}\".format(len(tf_idf_all_docs[12])))\n",
    "for key, value in list(tf_idf_all_docs[15].items())[20:25]:\n",
    "    print(key,value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Dimensions: 712\n"
     ]
    }
   ],
   "source": [
    "#Visualize vectorization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.004492843304019021,\n",
       " 0.14265391778828526,\n",
       " 0.04347364786264239,\n",
       " 0.0025297342356202382,\n",
       " 0.022562213725711686,\n",
       " 0.17831739723535658,\n",
       " 0.01160078963280243,\n",
       " 0.00592433487658073,\n",
       " 0.035663479447071315,\n",
       " 0.005338213012700637]"
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Reduce dimentionality\n",
    "tf_idf_vals_list = []\n",
    "\n",
    "for i in tf_idf_all_docs:\n",
    "    tf_idf_vals_list.append(list(i.values()))\n",
    "    \n",
    "tf_idf_vals_list[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.manifold import TSNE\n",
    "from nltk.tokenize import word_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "np.random.seed(10)\n",
    "\n",
    "tf_idf_all_docs = main(filenames) \n",
    "TSNE_2d = TSNE(n_components=2)\n",
    "data_2d = TSNE_2d.fit_transform(tf_idf_vals_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "TheBeatle = data_2d[:10]\n",
    "beatle_x = [i[0] for i in TheBeatle]\n",
    "beatle_y = [i[1] for i in TheBeatle]\n",
    "\n",
    "TaylorSwift = data_2d[10:]\n",
    "ts_x = [i[0] for i in TaylorSwift]\n",
    "ts_y = [i[1] for i in TaylorSwift]\n",
    "\n",
    "fig = plt.figure(figsize=(20,10))\n",
    "ax = fig.add_subplot(222)\n",
    "ax.scatter(beatle_x, beatle_y, color='g', label='The Beatle')\n",
    "ax.scatter(ts_x, ts_y, color='r', label='Taylor Swift')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
